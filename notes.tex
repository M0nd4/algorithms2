
\documentclass{article}
\usepackage{amsmath, amsfonts}

\begin{document}

\section{Greedy Algorithms}

\textbf{General Definition:} \\
Greedy algorithms make ``myopic'' decisions, meaning that make choices
on the first pass through and hope that everything works out in the end. An example is \textbf{Dijkstra's shortest path algorithm} - there was one
main while loop so the algorithm had only one shot to process all the
nodes.
\\
Generally, analyzing running times for greedy algorithms is much easier
than with divide and conquer, however analyzing correctness can be
quite tricky.  Consequently, they are often not correct!!  An example
of greedy algorithms being incorrect is Dijkstra's with negative 
edge lengths, it will greedily take the shortest path, ignoring that
future negative edge lengths could make other paths shorter.

\subsection{Proofs of Correctness for Greedy Algorithms}
\
\begin{description}
\item[$\bullet$ Method 1: ]{Proof by induction.  An example is the 
      proof of correctness for Dijkstra's algorithm.}
  \item[$\bullet$ Method 2: ]{Exchange argument - \textbf{more later}}
\item[$\bullet$ Method 3: ]{Whatever works - LOL.}
\end{description}

\section{A Scheduling Problem}
\
\begin{description}
\item[$\bullet$ Setup: ]{One shared resource (e.g, a processor) and 
  many jobs to do (e.g. process).}
\item[$\bullet$ Question: ]{In what order should we sequence the jobs?}
\item[$\bullet$ Assume: ]{Each job $j$ has a weight $w_j$ (``priority'')
    and a length $l_j$.}
\item[$\bullet$ Objective functions: ]{Minimize the weighted sum of the
    completion times.}
\end{description}
What are completion times? The \textbf{completion time} is the sum of job lengths up to and including the target job.

\subsection{A Greedy Algorithm}
Orders jobs by decreasing ratio $\frac {w_j}{l_j}$ (weight of job/
length of job).
\begin{description}
  \item[$\bullet$]{}
\end{description}

\section{Minimum spanning tree algorithms}
The idea is to connect a bunch of points as cheaply as possible.
Examples are clustering or networking.

\
\begin{description}
\item[$\bullet$ Input: ]{undirected graph G = (V, E) where V is vertices and E is edges.}
\item[$\bullet$ ]{Assume graph is given as an adjacency list representation.}
\item[$\bullet$ ]{Edge costs can be negative.}
\item[$\bullet$ Output: ]{Minimum cost tree $T \leq E$ that spans all vertices. I.e $T$ has no cycles and the subgraph (V,T) is connected (there is a path from any vertex to all other vertexes.)}
\item[$\bullet$ Assumptions to make things easier:]{
\
\begin{itemize}
\item[$\bullet$]{Input graph G is connected. Note this is easy to check in a pre-processing step (DFS).}
\item[$\bullet$]{In input graph edge costs are distinct. Note that Prim and Kruskal algorithms remain correct however when ties are allowed, we are just ignoring them to make things easier for now.}
\end{itemize}}
\end{description}

\subsection{Prim's Algorithm}
Similar to Dijkstra's (who also discovered it later).
Almost linear time $\Rightarrow O(m\log(n))$ time, where is $m$ is the
number of edges in a graph and $n$ is the number of vertices.
\
\begin{description}
\item[$\bullet$]{With each iteration adds one edge.}
\item[$\bullet$]{Its greedy, so it picks the edge with the smallest weight with each iteration.}
\end{description}

\textbf{The algorithm} (pseudocode):
\
\begin{enumerate}
  \item Initialize $X = [s]$, where X will be a set of vertices that we have spanned so far. $s$ is an arbitrary vertex, $s \in V$.
  \item $T = 0$, where $T$ is our tree, one edge will be added with each iteration.  Here we have an invariant: $X =$ vertices spanned by tree-so-far $T$. 
  \item The main while loop, each iteration is responsible for claiming one new edge crossing the 'frontier' greedily (what is the cheapest edge to claim).
  \item while $X \neq V:$
    \
    \begin{enumerate}
      \item let $e = (u,v)$ be the cheapest edge of $G$ (the graph) such that $u \in X, v \text{ not} \in X$.
      \item add $e$ to $T$
      \item add $v$ to $X$
    \end{enumerate}
\end{enumerate}

\subsection{Kruskal's Algorithm}
Almost linear time $\Rightarrow O(m\log(n))$ time.  Unlike Prim's algorithm, it just picks the cheapest edge from
all the remaining edges.

\textbf{The algorithm} (pseudocode):
\
\begin{enumerate}
\item Sort the edges in order of increasing cost.
\item Initialize T = 0, the tree in progress
\item for i = 1 to m:
\
\begin{enumerate}
\item - if T U {i} has no cycles:
\item - add i to T
\end{enumerate}
\item return T
\end{enumerate}

\section{Clustering}
Informal goal: Given $n$ 'points' classify them into coherent groups.
\\ 
Assumptions: 
\
\begin{enumerate}
\item As input, we are given a (dis)similarity measure - a \textbf{distance} $d(p,q)$ between each point pair.
\item Symmetric (i.e. $d(p,q) = d(p,q)$)
\item Examples: euclidean distance or some other norm measure, genome similarity, etc.
\item Goal: So things that are similar should be in the same group while dissimilar items should be in different groups, pretty 
straightforward.
\end{enumerate}
There are many ways to approach clustering, we are going to adopt a specific objective function to look at more closely, the 
\textbf{Max-Spacing k-Clusterings}.
\
\begin{enumerate}
\item Assume: we know k = \# of clusters desired (we know how many clusters we want)
\item Points are considered separated if they are in different groups.
\item Definition: The spacing of a k-clustering is min separated $d(p,q)$.  So, the algorithm's objective is to make all the
separated points be as far apart as possible, to maximize the distance between groups (the bigger the better).
\item Problem Statement: given a distance measure $d$ and $k$, compute the k-clustering with maximum spacing.
\end{enumerate}

\subsection{A Greedy Algorithm for k-Clustering}
We start with every point in its own cluster.  Start looking at the most alarming pair, i.e. the closest pair.  The only thing to 
do here is to fuse them into the same cluster. \\
\textbf{Pseudocode:}
\
\begin{enumerate}
  \item -initially, each point is in a separate cluster
\item - repeat until only k clusters remain:
\item - let p,q = the closest pair of separated points (determines the current spacing)
\item - merge the clusters containing p,q into a single cluster
\end{enumerate}
Note: this is just like Kruskal's MST algorithm, but aborted early (i.e. when k clusters remain), and points = vertices,
distances = edge costs.

\section{Huffman Codes}
Make variable length encoding to take advantage of the fact that in many cases certain characters appear more frequently.
Thus, encoding those characters with fewer bits can save the overall number of bits required for encoding (applied if compression, 
mp3s, genomes, etc).
\begin{itemize}
\item Goal: come up with the best binary prefix-free encoding for a given set of character frequencies.
\item Useful fact: Think of binary codes as binary trees (right child labeled 1, left child 0, nodes labeled with characters from
encoded alphabet)
\item In a prefix free code, only leaves can be assigned characters to encode (no characters are ancestors of others)
\item To decode, just follow path until you reach a node with a character, i.e. a leaf in the prefix-free scenario
\item The number of bits required to encode is a function of the depth of the tree
\end{itemize}
The problem definition:
\
\begin{itemize}
\item Input: probability $p_i$ for each character $i \in \Sigma$, the alphabet of interest.
\item Notation: if T = a binary tree that has leaves with one to one correspondence with the symbols of $\Sigma$,
then the average encoding length, $L(T) = \sum_{i\in\Sigma}p_i - [\text{depth of }i \text{ in } T]$.  In other words, 
the average encoding length is the weighted average of the depths of the binary encoding tree.
\end{itemize}
Huffman's idea was to build the tree from the bottom up, starting at the leaves, and successively merging groups together.
Leaves that are merged first are encoded by the most bits (i.e. with each merging, every leave in the merged groups picks
up a new bit as part of its encoding, so leaves merged last are encoded by the fewest bits).

\subsection{A greedy algorithm}
Given frequencies $p_i$ as input.  Let $\Sigma$ be out alphabet we want to encode.
\begin{itemize}
\item If $|\Sigma| = 2$, return a tree with just two leaves.
\item else:
\
\begin{itemize}
\item Let a,b $\in \Sigma$ have the smallest frequencies
\item Let $\Sigma' = \Sigma$ with a,b replaced by new 'meta' symbol 'ab'.
\item Define $p_{ab} = p_a + p_b$.
\item Recursively compute T' (for the alphabet $\Sigma'$)
\end{itemize}
\item Extend T' (with leaves from $\Sigma'$) to a tree T (with leaves from $\Sigma$) by splitting leaf 'ab' into 'a' and 'b' again
\item Return T
\end{itemize}

\subsection{Speed-ups}
The naive implementation above has $O(n^2)$ running time, where $n = |\Sigma|$.
Speed-ups:
\
\begin{itemize}
\item Use a heap to perform the repeated minimum computations.
\item Use keys = frequencies
\item after extracting the two smallest frequency symbols, re-insert the new meta-symbol back into the heap with
  the new key = the sum of their frequencies.
\item This now goes to $O(n\log n)$ time.
\end{itemize}
Even faster:
\
\begin{itemize}
\item Can be implemented with one sort and linear work with two queues!
\end{itemize}

\end{document}

